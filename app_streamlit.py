import streamlit as st
import numpy as np
import cv2
import pandas as pd
from tensorflow.keras.models import load_model
from streamlit_webrtc import webrtc_stream, VideoProcessorBase, WebRtcMode

# --- –ö–û–ù–°–¢–ê–ù–¢–´ ---
MODEL_PATH = 'traffic_sign_classifier_final_model_CLEAN.h5'
LABEL_FILE = 'labels.csv' 
IMG_SIZE = (32, 32)
THRESHOLD = 0.70 # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–Ω–∞–∫–∞

# --- 1. –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò –ò –ú–ï–¢–û–ö (–ö–≠–®–ò–†–£–ï–¢–°–Ø) ---
@st.cache_resource
def load_resources():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –º–µ—Ç–∫–∏, –∫—ç—à–∏—Ä—É—è –∏—Ö."""
    try:
        model = load_model(MODEL_PATH)
        st.success("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
    except Exception as e:
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
        st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª '{MODEL_PATH}' –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞. –û—à–∏–±–∫–∞: {e}")
        return None, None

    try:
        data = pd.read_csv(LABEL_FILE)
        sign_names = data['Name'].tolist()
    except FileNotFoundError:
        st.warning(f"‚ö†Ô∏è –§–∞–π–ª {LABEL_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è ID –∫–ª–∞—Å—Å–æ–≤.")
        sign_names = [f"Class {i}" for i in range(58)]
        
    return model, sign_names

# --- 2. –§–£–ù–ö–¶–ò–Ø –ü–†–ï–û–ë–†–ê–ë–û–¢–ö–ò (–î–õ–Ø –ú–û–î–ï–õ–ò) ---
def preprocess_for_prediction(img):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ —Å–µ—Ä—ã–π, —Ä–µ—Å–∞–π–∑–∏—Ç –¥–æ 32x32, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å."""
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–µ—Ä—ã–π —Ü–≤–µ—Ç, –µ—Å–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ü–≤–µ—Ç–Ω–æ–µ
    if img.ndim == 3 and img.shape[-1] == 3:
        # WebRTC –∫–∞–¥—Ä—ã –ø—Ä–∏—Ö–æ–¥—è—Ç –≤ BGR, –Ω–æ cvtColor —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –Ω–∞–¥–æ
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    img = cv2.resize(img, IMG_SIZE)
    img = img / 255.0 
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–ª—è Keras: (1, 32, 32, 1)
    img = np.expand_dims(img, axis=0) 
    img = np.expand_dims(img, axis=-1)
    return img

# --- 3. –ö–õ–ê–°–° –û–ë–†–ê–ë–û–¢–ö–ò –í–ò–î–ï–û–ü–û–¢–û–ö–ê ---
class TrafficSignProcessor(VideoProcessorBase):
    def __init__(self, model, sign_names):
        self.model = model
        self.sign_names = sign_names
        
    def recv(self, frame):
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∫–∞–¥—Ä–∞ –∏–∑ WebRTC –≤ –º–∞—Å—Å–∏–≤ numpy (BGR)
        img = frame.to_ndarray(format="bgr24")
        
        # 1. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        img_processed = preprocess_for_prediction(img.copy()) 
        
        # 2. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        predictions = self.model.predict(img_processed, verbose=0)
        class_index = np.argmax(predictions)
        probability = np.max(predictions)

        # 3. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        if probability > THRESHOLD:
            sign_label = self.sign_names[class_index]
            # –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ 100 –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
            text = f"{sign_label} ({probability*100:.2f}%)" 
            color = (0, 255, 0) # BGR: –ó–µ–ª–µ–Ω—ã–π
        else:
            text = "Searching..."
            color = (0, 0, 255) # BGR: –ö—Ä–∞—Å–Ω—ã–π
        
        # –ù–∞–ª–æ–∂–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∫–∞–¥—Ä OpenCV
        cv2.putText(img, text, (20, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∫–∞–¥—Ä –æ–±—Ä–∞—Ç–Ω–æ –≤ Streamlit
        return img


# --- 4. –û–°–ù–û–í–ù–û–ï –ü–†–ò–õ–û–ñ–ï–ù–ò–ï STREAMLIT ---

def main():
    st.set_page_config(page_title="Traffic Sign Detector", layout="wide")
    st.title("üöó –î–µ—Ç–µ–∫—Ç–æ—Ä –î–æ—Ä–æ–∂–Ω—ã—Ö –ó–Ω–∞–∫–æ–≤ (CNN) –≤ Web")
    st.markdown("–ü—Ä–æ–µ–∫—Ç –ø–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—é 58 –∫–ª–∞—Å—Å–æ–≤ –¥–æ—Ä–æ–∂–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤.")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ—Å—É—Ä—Å—ã
    model, sign_names = load_resources()

    if model is None:
        st.stop() # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞

    st.header("–í–∏–¥–µ–æ–ø–æ—Ç–æ–∫ —Å –ö–∞–º–µ—Ä—ã")
    
    # –ó–∞–ø—É—Å–∫ WebRTC —Å—Ç—Ä–∏–º–∞
    webrtc_stream(
        key="traffic-sign-detector",
        mode=WebRtcMode.SENDRECV,
        # –§–∞–±—Ä–∏–∫–∞ —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        video_processor_factory=lambda: TrafficSignProcessor(model, sign_names),
        async_processing=True,
        media_stream_constraints={"video": True, "audio": False},
        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å—Ç–∞—Ç—É—Å–µ
        rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
    )

    st.markdown("---")
    st.info(f"–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ 58 –∫–ª–∞—Å—Å–∞—Ö, –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ —Ç–æ—á–Ω–æ—Å—Ç—å: **92.21%**.")

if __name__ == "__main__":
    main()